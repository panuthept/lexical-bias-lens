import random
from tqdm import tqdm
from lens import LexicalBiasLens


class LexicalBiasExploitator(LexicalBiasLens):
    def permute(
        self, 
        samples: list, 
        target_label: str,
        top_k: int = 10,
        n: int = 1,
    ):
        assert self.bias_profile is not None, "Please create_profile() before permuting samples."
        assert target_label in self.available_labels, f"target_label '{target_label}' not found in available labels: {self.available_labels}"
        assert (isinstance(n, int) and n > 0), "n must be a positive integer."

        bias_tokens = self.find_bias_keywords(target_label=target_label, top_k=top_k, ranking_order="decending")
        bias_tokens = [token for token, score in bias_tokens]

        permuted_samples = samples.copy()

        for i in range(len(permuted_samples)):
            tokens = permuted_samples[i].copy()
            permuted_tokens = tokens
            suffix_tokens = [random.choice(bias_tokens) for _ in range(n)]
            for j in range(len(suffix_tokens)):
                permuted_tokens = [*permuted_tokens, *suffix_tokens[j]]
            permuted_samples[i] = permuted_tokens
        return permuted_samples


if __name__ == "__main__":
    import os
    from datasets import load_dataset
    from transformers import AutoTokenizer

    save_path = "saves/aisingapore/1M_SEA-Guard_qwen3-8b_Non_Bias/train_refined_v1_qa_pass_only"
    tokenizer = AutoTokenizer.from_pretrained("aisingapore/1M_SEA-Guard_qwen3-8b_Non_Bias")

    dataset_sources = [
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Indonesia", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Malaysia", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Myanmar", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Philippines", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Singapore", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Thailand", "train_refined_v1_qa_pass_only"),
        ("aisingapore/SEA-Safeguard-Train-Cultural-v3", "Vietnam", "train_refined_v1_qa_pass_only"),
    ]

    labels = []
    samples = []
    for repo, subset, split in dataset_sources:
        dataset = load_dataset(repo, subset, split=split)
    
        for sample in tqdm(dataset):
            if sample["response"] is None:
                input = sample['prompt']
                label = sample['prompt_label']
            else:
                input = (sample['prompt'] + "\n" + sample['response'])
                label = sample['response_label']
            tokens = tokenizer.tokenize(input)
            samples.append(tokens)
            labels.append(label)
    print(f"Loaded {len(samples)} samples.")

    if not os.path.exists(save_path):
        exploitator = LexicalBiasExploitator(max_n=1, metric="LMI")
        exploitator.create_profile(samples, labels)
        exploitator.save(save_path)
    else:
        exploitator = LexicalBiasExploitator.load(save_path, metric="LMI")

    unsafe_samples = [sample for sample, label in zip(samples, labels) if label == "Unsafe"]
    before_pred_labels = exploitator.predict(samples=unsafe_samples)

    permuted_samples = exploitator.permute(unsafe_samples, target_label="Safe", n=5, top_k=1)

    after_pred_labels = exploitator.predict(samples=permuted_samples)
    for i in range(10):
        print("Original Sample:", " ".join(unsafe_samples[i]))
        print("Original Label:", labels[i])
        print("Predicted Label Before Permutation:", before_pred_labels[i])
        print("Permuted Sample:", " ".join(permuted_samples[i]))
        print("Predicted Label After Permutation:", after_pred_labels[i])
        print("-----")